<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta
      name="description"
      content="Chain-of-Thought Hijacking: A jailbreak attack on reasoning models that achieves state-of-the-art success rates"
    />
    <title>Chain-of-Thought Hijacking</title>

    <!-- Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&family=JetBrains+Mono:wght@400;500&display=swap"
      rel="stylesheet"
    />

    <!-- Styles -->
    <link rel="stylesheet" href="css/style.css" />

    <!-- Chart.js for visualizations -->
    <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.0/dist/chart.umd.min.js"></script>
  </head>
  <body>
    <!-- Navigation -->
    <nav class="navbar" id="navbar">
      <div class="container nav-container">
        <div class="nav-brand">CoT Hijacking</div>
        <ul class="nav-menu" id="nav-menu">
          <li><a href="#home" class="nav-link">Home</a></li>
          <li><a href="#abstract" class="nav-link">Abstract</a></li>
          <li><a href="#results" class="nav-link">Results</a></li>
          <li><a href="#methodology" class="nav-link">Method</a></li>
          <li><a href="#analysis" class="nav-link">Analysis</a></li>
          <li><a href="#citation" class="nav-link">Citation</a></li>
        </ul>
        <button class="nav-toggle" id="nav-toggle">
          <span></span>
          <span></span>
          <span></span>
        </button>
      </div>
    </nav>

    <!-- Hero Section -->
    <section id="home" class="hero">
      <div class="hero-background"></div>
      <div class="container hero-content" style="margin-top: -3rem;">
        <div class="hero-badge">Preprint</div>
        <h1 class="hero-title" style="margin-top: 1rem; margin-bottom: 2rem;">
          Chain-of-Thought Hijacking
          <img
            src="assets/Jailbreak_Icon_Color.png"
            alt="🔓"
            class="title-icon"
            style="
              width: 3.5rem;
              height: 3.5rem;
              display: inline-block;
              vertical-align: middle;
            "
          />
        </h1>
        <p class="hero-description" style="font-size: 1.2rem; line-height: 1.6;">
          We introduce CoT Hijacking, which achieves
          <strong>99% attack success rate</strong> on frontier <span style="white-space: nowrap;">Large Reasoning Models</span><br>
          by padding harmful requests with long sequences of benign puzzle reasoning.
        </p>
        <div class="hero-buttons">
          <a
            href="assets/Hijacking_paper.pdf"
            class="btn btn-primary"
            target="_blank"
          >
            📄 Read Paper
          </a>
          <a
            href="https://github.com/girlsuuu/Hijacking"
            class="btn btn-secondary"
            target="_blank"
          >
            💻 GitHub Code
          </a>
        </div>

        <!-- Authors on Hero Page -->
        <div class="hero-authors" style="margin-top: 40px; padding-top: 30px; border-top: 1px solid rgba(255, 255, 255, 0.1); max-width: 95%; margin-left: auto; margin-right: auto;">
          <div style="display: flex; justify-content: center; gap: 50px; margin: 0 auto;">
            <div style="text-align: center;">
              <div style="font-size: 1.05rem; font-weight: 500; margin-bottom: 6px; white-space: nowrap;">Jianli Zhao</div>
              <div style="font-size: 0.9rem; opacity: 0.8; white-space: nowrap;">Independent</div>
            </div>
            <div style="text-align: center;">
              <div style="font-size: 1.05rem; font-weight: 500; margin-bottom: 6px; white-space: nowrap;">Tingchen Fu</div>
              <div style="font-size: 0.9rem; opacity: 0.8; white-space: nowrap;">Renmin University of China</div>
            </div>
            <div style="text-align: center;">
              <div style="font-size: 1.05rem; font-weight: 500; margin-bottom: 6px; white-space: nowrap;">Rylan Schaeffer</div>
              <div style="font-size: 0.9rem; opacity: 0.8; white-space: nowrap;">Stanford University</div>
            </div>
            <div style="text-align: center;">
              <div style="font-size: 1.05rem; font-weight: 500; margin-bottom: 6px; white-space: nowrap;">Mrinank Sharma</div>
              <div style="font-size: 0.9rem; opacity: 0.8; white-space: nowrap;">Anthropic</div>
            </div>
            <div style="text-align: center;">
              <div style="font-size: 1.05rem; font-weight: 500; margin-bottom: 6px; white-space: nowrap;">Fazl Barez</div>
              <div style="font-size: 0.9rem; opacity: 0.8; white-space: nowrap;">University of Oxford · WhiteBox · Martian</div>
            </div>
          </div>
        </div>
      </div>
      <div class="scroll-indicator">
        <span>Scroll to explore</span>
        <div class="scroll-arrow"></div>
      </div>
    </section>

    <!-- Abstract -->
    <section id="abstract" class="section section-alt">
      <div class="container">
        <h2 class="section-title">Abstract</h2>
        <div class="abstract-content">
          <p>
            Large reasoning models (LRMs) achieve higher task performance by
            allocating more inference-time compute, and prior works suggest this
            scaled reasoning may also strengthen safety by improving refusal.
            Yet we find the opposite: the same reasoning can be used to bypass safety.
          </p>
          <p>
            We introduce <em>Chain-of-Thought Hijacking</em>, a jailbreak attack
            on reasoning models. The attack pads harmful requests with long
            sequences of harmless reasoning. Across HarmBench, CoT Hijacking
            reaches a <strong>99%, 94%, 100%, and 94%</strong> attack success
            rate (ASR) on Gemini 2.5 Pro, GPT o4 mini, Grok 3 mini, and Claude 4
            Sonnet, respectively—far exceeding prior jailbreak methods for LRMs.
          </p>
          <p>
            To understand the effectiveness of our attack, we turn to a
            mechanistic analysis, which shows that mid layers encode the
            <em>strength of safety checking</em>, while late layers encode the
            <em>verification outcome</em>. Long benign CoT dilutes both signals
            by shifting attention away from harmful tokens. Targeted ablations
            of attention heads identified by this analysis causally decrease
            refusal, confirming their role in a safety subnetwork.
          </p>
          <p>
            These results show that the most interpretable form of reasoning—explicit
            CoT—can itself become a jailbreak vector when combined with final-answer cues.
            We release prompts, outputs, and judge decisions to facilitate replication.
          </p>
        </div>
      </div>
    </section>

    <!-- Results -->
    <section id="results" class="section section-alt">
      <div class="container">
        <h2 class="section-title">Experimental Results</h2>

        <!-- Chart Visualization -->
        <div class="chart-container">
          <canvas id="resultsChart"></canvas>
        </div>
        <p class="section-description" style="margin-top: 30px; text-align: center;">
          We evaluated CoT Hijacking on 100 HarmBench samples across four
          frontier reasoning models, comparing against state-of-the-art baseline
          jailbreak methods
        </p>
      </div>
    </section>

    <!-- Methodology -->
    <section id="methodology" class="section">
      <div class="container">
        <h2 class="section-title">Attack Methodology</h2>
      </div>

      <div class="method-overview" style="display: flex; gap: 50px; align-items: center; max-width: 80%; margin: 40px auto 0; padding: 0 20px;">
        <!-- Attack Overview Figure -->
        <div class="figure-container" style="flex: 1; margin-bottom: 0;">
          <img
            src="assets/Jailbreaking_Models_Fig02.png"
            alt="Attack Overview"
            class="figure-img"
            style="width: 100%; max-width: none;"
          />
          <p class="figure-caption" style="margin-top: 20px;">
            <strong>Figure 1: Safe vs. Jailbreak Examples.</strong>
            The upper part illustrates a safe example where the target model
            refuses a harmful request. The lower part shows a successful
            jailbreak where the target model complies under our attack. Grey
            highlights indicate puzzle content, red highlights mark
            malicious requests.
          </p>
        </div>

        <div class="method-diagram" style="flex: 1; display: flex; flex-direction: column; justify-content: center;">
          <img
            src="assets/Jailbreaking_Models_Fig03_v2.png"
            alt="Attack Pipeline"
            class="figure-img"
            style="width: 100%; max-width: none;"
          />
          <p class="figure-caption" style="margin-top: 20px;">
            <strong>Figure 2: Jailbreak Method Pipeline.</strong>
            The upper part illustrates the process of generating our jailbreak
            query, while the lower part shows how the target model is
            attacked. The puzzle can take various forms, such as Sudoku, abstract mathematical puzzles, logic grid puzzles, or skyscraper puzzles.
          </p>
        </div>
      </div>
    </section>

    <!-- Mechanistic Analysis -->
    <section id="analysis" class="section section-alt">
      <div class="container">
        <h2 class="section-title">Mechanistic Analysis</h2>
        <p class="section-description">
          We analyze <em>why</em> CoT Hijacking works through refusal direction
          experiments, attention pattern analysis, and causal interventions on
          attention heads.
        </p>

        <!-- Attention Analysis -->
        <div class="analysis-subsection" style="margin-top: 50px">
          <h3>Attention Pattern Analysis</h3>
          <p>
            Longer CoT sequences consistently reduce refusal components in the later layers.
            As CoT length increases, attention systematically shifts away from harmful tokens, diluting the safety mechanisms that detect and refuse unsafe requests.
          </p>
          <div class="figures-grid">
            <div class="figure-item">
              <img
                src="assets/overlaid_layer_comparison_3.png"
                alt="Refusal Components"
                class="analysis-img"
              />
              <p class="figure-caption">
                <strong>Figure 3: Refusal Components Across Layers for Different CoT Lengths.</strong>
                Longer reasoning diminishes the refusal signal.
              </p>
            </div>
            <div class="figure-item">
              <img
                src="assets/attention_ratio_trend_1.png"
                alt="Attention Ratio"
                class="analysis-img"
              />
              <p class="figure-caption">
                <strong>Figure 4: Attention Ratio vs CoT Length.</strong>
                Longer CoT reduces attention to harmful instructions.
              </p>
            </div>
          </div>
        </div>

        <!-- Key Insights -->
        <div class="insights-box">
          <h3>🔍 Mechanistic Insights</h3>
          <ul class="insights-list">
            <li>
              <strong>Low-dimensional refusal:</strong> Safety checks rely on a
              single direction in activation space, which can be bidirectionally
              controlled.
            </li>
            <li>
              <strong>Refusal dilution:</strong> Long benign reasoning dilutes
              the refusal signal by shifting attention away from harmful tokens
              (only ~5% of context).
            </li>
            <li>
              <strong>Safety subnetwork:</strong> Specific attention heads in
              layers 15-35 are causally responsible for safety checking.
              Ablating them eliminates refusals.
            </li>
            <li>
              <strong>Toward mitigation:</strong> Effective defenses require
              deeper integration of safety into reasoning itself—monitoring
              refusal activation across layers, penalizing dilution, or
              enforcing attention to harmful spans regardless of reasoning length.
            </li>
          </ul>
        </div>
      </div>
    </section>

    <!-- Citation -->
    <section id="citation" class="section section-alt">
      <div class="container">
        <h2 class="section-title">Citation</h2>
        <div class="citation-box">
          <pre id="bibtex-content">
@article{To be arxived...
  ...
  ...
  ...
}</pre
          >
          <button class="btn-copy" id="copy-bibtex">📋 Copy BibTeX</button>
        </div>
      </div>
    </section>

    <!-- Footer -->
    <footer class="footer">
      <div class="container">
        <div class="footer-content">
          <div class="footer-section">
            <h4>⚠️ Safety Notice</h4>
            <p>
              This research is for
              <strong>defensive security purposes only</strong>.<br />
              Do not use these methods for malicious purposes. We release this
              work to help the community build more robust safety mechanisms.
            </p>
          </div>
        </div>
        <div class="footer-bottom">
          <p>&copy; 2026 Chain-of-Thought Hijacking. All rights reserved.</p>
        </div>
      </div>
    </footer>

    <!-- Back to Top Button -->
    <button class="back-to-top" id="back-to-top">↑</button>

    <!-- Scripts -->
    <script src="js/app.js"></script>
  </body>
</html>
